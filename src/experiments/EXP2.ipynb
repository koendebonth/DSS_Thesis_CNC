{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Problem of unbalanced data\n",
    "\n",
    "### One class classification vs Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    from sklearn.ensemble import IsolationForest,RandomForestClassifier\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    # Custom utilities\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.load_data import load_data\n",
    "    from utils.result_utils import create_results_df, record_result\n",
    "\n",
    "\n",
    "    from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report,accuracy_score,precision_score,recall_score\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 == good | 1 == bad |\n",
    "X, y, y_binary = load_data()\n",
    "\n",
    "result_df_RF = create_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(m01, m02, m03,show_confusion_matrix=False):\n",
    "    '''Run one-class SVM experiment with specified machine fractions.'''   \n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "    machine_train_frac = {'M01': m01, 'M02': m02, 'M03': m03}\n",
    "    trainX, trainy, testX, testy = [], [], [], []\n",
    "    \n",
    "    for machine, frac in machine_train_frac.items():\n",
    "        # filter samples for this machine\n",
    "        data = [(x_i, y_i) for x_i, y_i in zip(Xtrain, ytrain)]\n",
    "        X_m = [d[0] for d in data if d[1].split('_')[0] == machine]\n",
    "        y_m = [0 if d[1].split('_')[-1] == 'good' else 1 for d in data if d[1].split('_')[0] == machine]\n",
    "        if frac == 1.0:\n",
    "            trainX.extend(X_m); trainy.extend(y_m)\n",
    "        elif frac == 0.0:\n",
    "            testX.extend(X_m); testy.extend(y_m)\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_m, y_m, train_size=frac, stratify=y_m, random_state=42\n",
    "            )\n",
    "            trainX.extend(X_tr); trainy.extend(y_tr)\n",
    "            testX.extend(X_te); testy.extend(y_te)\n",
    "\n",
    "    # transform and resample\n",
    "    trainX_tr, trainy_tr = transform_data(trainX, trainy)\n",
    "    testX_tr, testy_tr = transform_data(Xtest, ytest,label_type='string')\n",
    "\n",
    "    # print(f\"Class distribution before resampling: {pd.Series(trainy_tr).value_counts()}\")\n",
    "    \n",
    "    rus = RandomUnderSampler(sampling_strategy=0.25, random_state=42)\n",
    "\n",
    "    trainX_tr, trainy_tr = rus.fit_resample(trainX_tr, trainy_tr)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    trainX_tr_resampled, trainy_tr_resampled = smote.fit_resample(trainX_tr, trainy_tr)\n",
    "    \n",
    "    RF = RandomForestClassifier(max_features='log2', \n",
    "                                n_estimators=150,\n",
    "                                max_depth=15,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_samples_split=2,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    RF.fit(trainX_tr_resampled, trainy_tr_resampled)\n",
    "\n",
    "    yhat = RF.predict(testX_tr)\n",
    "    score = f1_score(testy_tr, yhat, pos_label=1, average='binary')\n",
    "    cm = confusion_matrix(testy_tr, yhat)\n",
    "\n",
    "    record_result(result_df_RF, m01, m02, m03, trainy, trainy_tr_resampled, testy, score, cm)\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        return ConfusionMatrixDisplay.from_estimator(RF, testX_tr, testy_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. ÉÉN VASTE PIPELINE – hyper-parameters invriezen\n",
    "# ------------------------------------------------------------------\n",
    "def build_fixed_pipeline(best_params, random_state=42):\n",
    "    \"\"\"Maak een RUS → SMOTE → RandomForest pipeline met vastgezette params.\"\"\"\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=best_params[\"rus__sampling_strategy\"],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    sm = SMOTE(\n",
    "        sampling_strategy=best_params[\"smote__sampling_strategy\"],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    rf_params = {k.split(\"rf__\")[1]: v          # strip de voorvoegsels\n",
    "                 for k, v in best_params.items() if k.startswith(\"rf__\")}\n",
    "    rf = RandomForestClassifier(random_state=random_state, **rf_params)\n",
    "    return Pipeline([(\"rus\", rus), (\"smote\", sm), (\"rf\", rf)])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. HULPFUNCTIE – stel de trainingset samen voor één k-waarde\n",
    "# ------------------------------------------------------------------\n",
    "def assemble_train_set(X_src, y_src,\n",
    "                       X_tgt, y_tgt,\n",
    "                       k, rnd, op_ids_src=None, op_ids_tgt=None):\n",
    "    \"\"\"Voeg k 'bad' samples uit target toe + bijbehorende 'good' (optioneel).\"\"\"\n",
    "    # -- split target in bad / good indices ------------------------\n",
    "    idx_bad  = np.where(y_tgt == 1)[0]\n",
    "    idx_good = np.where(y_tgt == 0)[0]\n",
    "\n",
    "    # -- random trekkingen per bootstrap-rep -----------------------\n",
    "    rng = np.random.default_rng(rnd)\n",
    "    if k > len(idx_bad):\n",
    "        raise ValueError(f\"k = {k} groter dan #available bad samples ({len(idx_bad)})\")\n",
    "    sel_bad = rng.choice(idx_bad, size=k, replace=False)\n",
    "\n",
    "    # (optioneel) alle good van dezelfde operatie meepakken -----------\n",
    "    if op_ids_tgt is not None:\n",
    "        ops_chosen = np.unique(op_ids_tgt[sel_bad])\n",
    "        sel_good_extra = np.where(np.isin(op_ids_tgt, ops_chosen) & (y_tgt == 0))[0]\n",
    "        sel_good = sel_good_extra\n",
    "    else:                      # fallback: evenveel good random nemen\n",
    "        sel_good = rng.choice(idx_good, size=k*2, replace=False) if k else []\n",
    "    # ---------------------------------------------------------------\n",
    "    X_aug = np.concatenate([X_src,      X_tgt[sel_bad],  X_tgt[sel_good]])\n",
    "    y_aug = np.concatenate([y_src,      y_tgt[sel_bad],  y_tgt[sel_good]])\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. MAIN LOOP – één bootstrap-rep\n",
    "# ------------------------------------------------------------------\n",
    "def single_run(src, tgt,\n",
    "               X_src, y_src, X_tgt_pool, y_tgt_pool, X_test, y_test,\n",
    "               k, best_params, rep, op_ids=None, random_state=42):\n",
    "    rnd = random_state + rep      # ander seed per rep\n",
    "    pipe = build_fixed_pipeline(best_params, random_state=rnd)\n",
    "    # assemble train set\n",
    "    X_train, y_train = assemble_train_set(\n",
    "        X_src, y_src, X_tgt_pool, y_tgt_pool, k, rnd,\n",
    "        op_ids_src=None,\n",
    "        op_ids_tgt=None if op_ids is None else op_ids[tgt]\n",
    "    )\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    return f1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. EXPERIMENT – alle paren, alle k's, 20×\n",
    "# ------------------------------------------------------------------\n",
    "def run_cross_machine_experiment(machine_data,\n",
    "                                 best_params,\n",
    "                                 k_grid=(0,2,4,6,8,10,15,20),\n",
    "                                 n_reps=20,\n",
    "                                 test_size=0.3,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=42,\n",
    "                                 op_ids=None):\n",
    "    \"\"\"Return DataFrame met mediane F1's + n★ per (src→tgt).\"\"\"\n",
    "    records = []\n",
    "    # ----------------------------------------------------------------\n",
    "    for src, tgt in permutations(machine_data.keys(), 2):\n",
    "        X_src, y_src = machine_data[src]\n",
    "        X_tgt, y_tgt = machine_data[tgt]\n",
    "        # één vaste testset per (src→tgt)\n",
    "        X_tgt_pool, X_test, y_tgt_pool, y_test = train_test_split(\n",
    "            X_tgt, y_tgt, test_size=test_size, stratify=y_tgt,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # baseline-F1 (zonder cross-samples) -------------------------\n",
    "        baseline_pipe = build_fixed_pipeline(best_params, random_state)\n",
    "        baseline_pipe.fit(X_tgt_pool, y_tgt_pool)\n",
    "        baseline_pred = baseline_pipe.predict(X_test)\n",
    "        F1_baseline = f1_score(y_test, baseline_pred, average=\"macro\")\n",
    "        F1_target = 0.95 * F1_baseline\n",
    "\n",
    "        # alle k-waarden (Parallel voor snelheid) -------------------\n",
    "        for k in k_grid:\n",
    "            f1_vals = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(single_run)(\n",
    "                    src, tgt,\n",
    "                    X_src, y_src, X_tgt_pool, y_tgt_pool,\n",
    "                    X_test, y_test,\n",
    "                    k, best_params, rep,\n",
    "                    op_ids, random_state\n",
    "                )\n",
    "                for rep in range(n_reps)\n",
    "            )\n",
    "            median_f1 = float(np.median(f1_vals))\n",
    "            records.append({\n",
    "                \"source\": src, \"target\": tgt,\n",
    "                \"k\": k, \"median_F1\": median_f1,\n",
    "                \"F1_target\": F1_target,\n",
    "                \"meets_95pct\": median_f1 >= F1_target,\n",
    "                \"tot_bad_target\": int(np.sum(y_tgt == 1))\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # n★ berekenen ---------------------------------------------------\n",
    "    summary = []\n",
    "    for (src, tgt), grp in df.groupby([\"source\", \"target\"]):\n",
    "        ok = grp[grp[\"meets_95pct\"]].sort_values(\"k\")\n",
    "        n_star  = int(ok[\"k\"].iloc[0]) if not ok.empty else np.nan\n",
    "        pct_star = 100 * n_star / grp[\"tot_bad_target\"].iloc[0] if not ok.empty else np.nan\n",
    "        summary.append({\n",
    "            \"source\": src, \"target\": tgt,\n",
    "            \"F1_baseline\": grp[\"F1_target\"].iloc[0] / 0.95,\n",
    "            \"n_star\": n_star,\n",
    "            \"pct_bad\": pct_star,\n",
    "            \"median_F1_at_n_star\": ok[\"median_F1\"].iloc[0] if not ok.empty else np.nan\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    return df, summary_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. VOORBEELD VAN AANROEPEN ---------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "\n",
    "    # (1) laad of prepareer je data hier \n",
    "       machine_data = {\n",
    "           \"M01\": (X_M01, y_M01),\n",
    "           \"M02\": (X_M02, y_M02),\n",
    "           \"M03\": (X_M03, y_M03),\n",
    "       }\n",
    "    # (2) plak hier je best_params, bijv.:\n",
    "       best_params = {\n",
    "           'rus__sampling_strategy': 0.2,\n",
    "           'smote__sampling_strategy': 0.5,\n",
    "           'rf__n_estimators': 200,\n",
    "           'rf__max_depth': 15,\n",
    "           'rf__min_samples_split': 2,\n",
    "           'rf__min_samples_leaf': 1,\n",
    "           'rf__max_features': 'sqrt'\n",
    "       }\n",
    "    \n",
    "    df_all, df_summary = run_cross_machine_experiment(\n",
    "        machine_data, best_params,\n",
    "        k_grid=(0,2,4,6,8,10,15,20),\n",
    "        n_reps=20, n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Sample-efficiency resultaten ===\")\n",
    "    print(df_summary.to_string(index=False))\n",
    "    df_all.to_csv(\"exp2_full_results.csv\", index=False)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, y_binary = load_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_M01 = []\n",
    "X_M02 = []\n",
    "X_M03 = []\n",
    "y_M01 = []\n",
    "y_M02 = []\n",
    "y_M03 = []\n",
    "\n",
    "for i in range(len(y)):\n",
    "    machine = y[i].split(\"_\")[0]\n",
    "    is_good = 0 if y[i].split(\"_\")[-1] == \"good\" else 1\n",
    "    \n",
    "    if machine == 'M01':\n",
    "        X_M01.append(X[i])\n",
    "        y_M01.append(is_good)\n",
    "    elif machine == 'M02':\n",
    "        X_M02.append(X[i])\n",
    "        y_M02.append(is_good)\n",
    "    elif machine == 'M03':\n",
    "        X_M03.append(X[i])\n",
    "        y_M03.append(is_good)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_M01 = np.array(X_M01)\n",
    "X_M02 = np.array(X_M02)\n",
    "X_M03 = np.array(X_M03)\n",
    "y_M01 = np.array(y_M01)\n",
    "y_M02 = np.array(y_M02)\n",
    "y_M03 = np.array(y_M03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by machine\n",
    "X_M01 = X[X['machine'] == 'M01'].drop('machine', axis=1)\n",
    "X_M02 = X[X['machine'] == 'M02'].drop('machine', axis=1)\n",
    "X_M03 = X[X['machine'] == 'M03'].drop('machine', axis=1)\n",
    "\n",
    "# Create binary labels (0 for good, 1 for anomaly)\n",
    "y_M01 = np.array([0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y[X['machine'] == 'M01']])\n",
    "y_M02 = np.array([0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y[X['machine'] == 'M02']])\n",
    "y_M03 = np.array([0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y[X['machine'] == 'M03']])\n",
    "\n",
    "machine_data = {\n",
    "    \"M01\": (X_M01, y_M01),\n",
    "    \"M02\": (X_M02, y_M02),\n",
    "    \"M03\": (X_M03, y_M03),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from grid search\n",
    "best_model\n",
    "\n",
    "# Extract feature importances from the Random Forest classifier\n",
    "feature_importances = best_model.named_steps['rf'].feature_importances_\n",
    "feature_names = trainX_tr.columns\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance and display top 20 features\n",
    "top_features = importance_df.sort_values('Importance', ascending=False).head(20)\n",
    "print(\"Top 20 features selected by the model:\")\n",
    "display(top_features)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features['Feature'][:10], top_features['Importance'][:10])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "# Create path to models folder (going up from src to root, then to models)\n",
    "models_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'models')\n",
    "\n",
    "# Ensure the models directory exists\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save best model in the models folder\n",
    "model_path = os.path.join(models_path, 'EXP1_BINARY_MODEL.pkl')\n",
    "pickle.dump(best_model, open(model_path, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(model_path,'rb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
