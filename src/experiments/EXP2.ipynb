{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Cross-Machine Sample-Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "# Get the absolute path to the current notebook directory\n",
    "current_dir = Path().resolve()\n",
    "\n",
    "# Set the project root directory (two levels up from notebooks if in experiments folder)\n",
    "project_root = current_dir.parent.parent\n",
    "\n",
    "# Add the project root to sys.path so Python can find the utils module\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom utilities\n",
    "from utils.load_data import load_data\n",
    "from utils.feature_extraction import transform_data\n",
    "\n",
    "print(\"Dependencies loaded successfully ✅\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Hyper-parameters fixed from Experiment 1 (best RF)\n",
    "# --------------------------------------------------\n",
    "BEST_PARAMS: Dict[str, object] = {\n",
    "    \"rus__sampling_strategy\": 0.2,\n",
    "    \"smote__sampling_strategy\": 0.5,\n",
    "    \"rf__n_estimators\": 200,\n",
    "    \"rf__max_depth\": 15,\n",
    "    \"rf__min_samples_split\": 2,\n",
    "    \"rf__min_samples_leaf\": 1,\n",
    "    \"rf__max_features\": \"sqrt\",\n",
    "}\n",
    "\n",
    "K_GRID: Sequence[int] = (0, 2, 4, 6, 8, 10, 15, 20)\n",
    "N_REPS: int = 20\n",
    "TEST_SIZE: float = 0.3  # fraction of *target* kept for final test\n",
    "RANDOM_STATE: int = 42\n",
    "N_JOBS: int = -1\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "\n",
    "def _parse_label(label: str) -> Tuple[str, str, str]:\n",
    "    \"\"\"Return (machine, process, status) from full label string.\"\"\"\n",
    "    parts = label.split(\"_\")\n",
    "    return parts[0], parts[3], parts[-1]\n",
    "\n",
    "\n",
    "def _build_metadata(labels: Sequence[str]) -> pd.DataFrame:\n",
    "    \"\"\"Convert list of label strings to a metadata DataFrame.\"\"\"\n",
    "    meta = [dict(machine=m, process=p, status=s) for m, p, s in map(_parse_label, labels)]\n",
    "    return pd.DataFrame(meta)\n",
    "\n",
    "\n",
    "def _prepare_features() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"Load raw data, extract wavelet features and return features, y, meta.\"\"\"\n",
    "    X_raw, y_raw, _ = load_data()  # y_raw are full strings\n",
    "    meta = _build_metadata(y_raw)\n",
    "\n",
    "    # One-shot feature extraction for the full dataset\n",
    "    X_feat, y_num = transform_data(X_raw, y_raw, label_type=\"string\")\n",
    "    # Ensure consistent indices\n",
    "    X_feat.index = meta.index = np.arange(len(meta))\n",
    "    return X_feat, y_num, meta\n",
    "\n",
    "\n",
    "def _make_pipeline(best: Dict[str, object], random_state: int) -> Pipeline:\n",
    "    \"\"\"Instantiate the RF pipeline with fixed hyper-parameters.\"\"\"\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=best[\"rus__sampling_strategy\"], random_state=random_state\n",
    "    )\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=best[\"smote__sampling_strategy\"], random_state=random_state\n",
    "    )\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=best[\"rf__n_estimators\"],\n",
    "        max_depth=best[\"rf__max_depth\"],\n",
    "        min_samples_split=best[\"rf__min_samples_split\"],\n",
    "        min_samples_leaf=best[\"rf__min_samples_leaf\"],\n",
    "        max_features=best[\"rf__max_features\"],\n",
    "        n_jobs=N_JOBS,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return Pipeline([(\"rus\", rus), (\"smote\", smote), (\"rf\", rf)])\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Within-machine reference (for 95 % threshold)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def _within_machine_f1(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    meta: pd.DataFrame,\n",
    "    machine: str,\n",
    "    test_size: float,\n",
    "    random_state: int,\n",
    ") -> float:\n",
    "    \"\"\"Train/test on the same machine; return macro-F1 on hold-out.\"\"\"\n",
    "    idx = meta.index[meta[\"machine\"] == machine]\n",
    "    X_m, y_m = X.loc[idx], y.loc[idx]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_m, y_m, test_size=test_size, stratify=y_m, random_state=random_state\n",
    "    )\n",
    "    pipe = _make_pipeline(BEST_PARAMS, random_state)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    return f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Core experiment\n",
    "# --------------------------------------------------\n",
    "\n",
    "def run_cross_machine_experiment(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    meta: pd.DataFrame,\n",
    "    k_grid: Sequence[int] = K_GRID,\n",
    "    n_reps: int = N_REPS,\n",
    "    test_size: float = TEST_SIZE,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    baseline_override: float | None = None,      #  NEW\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (df_all, df_summary).\"\"\"\n",
    "    rng = random.Random(random_state)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Baseline(s)\n",
    "    # --------------------------------------------------\n",
    "    if baseline_override is None:\n",
    "        baseline_f1 = {\n",
    "            m: _within_machine_f1(X, y, meta, m, test_size, random_state)\n",
    "            for m in (\"M01\", \"M02\", \"M03\")\n",
    "        }\n",
    "    else:\n",
    "        # same baseline for every target machine\n",
    "        baseline_f1 = {m: baseline_override for m in (\"M01\", \"M02\", \"M03\")}\n",
    "\n",
    "    records: List[Dict[str, object]] = []\n",
    "\n",
    "    for source in (\"M01\", \"M02\", \"M03\"):\n",
    "        for target in (\"M01\", \"M02\", \"M03\"):\n",
    "            if source == target:\n",
    "                continue  # only cross-machine pairs\n",
    "\n",
    "            # Pre-select indices for faster masks\n",
    "            idx_source = meta.index[meta[\"machine\"] == source]\n",
    "            idx_target = meta.index[meta[\"machine\"] == target]\n",
    "            idx_target_bad = idx_target[meta.loc[idx_target, \"status\"] == \"bad\"]\n",
    "            idx_target_good = idx_target[meta.loc[idx_target, \"status\"] == \"good\"]\n",
    "\n",
    "            # Operations mapping (process → good indices)\n",
    "            proc_good_map: Dict[str, List[int]] = defaultdict(list)\n",
    "            for i in idx_target_good:\n",
    "                proc_good_map[meta.loc[i, \"process\"].upper()].append(i)\n",
    "\n",
    "            # List of bad indices per process\n",
    "            proc_bad_map: Dict[str, List[int]] = defaultdict(list)\n",
    "            for i in idx_target_bad:\n",
    "                proc_bad_map[meta.loc[i, \"process\"].upper()].append(i)\n",
    "\n",
    "            for k in k_grid:\n",
    "                for rep in range(n_reps):\n",
    "                    # ── NEW ────────────────────────────────────────────────────────────\n",
    "                    # keep at least one faulty sample for the test set; if impossible,\n",
    "                    # skip this (k, rep) configuration\n",
    "                    if k >= len(idx_target_bad):\n",
    "                        continue\n",
    "\n",
    "                    # draw the k faulty samples that will be added to the training set\n",
    "                    chosen_bad_idx = rng.sample(list(idx_target_bad), k) if k else []\n",
    "                    # ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "                    chosen_processes = {meta.loc[i, \"process\"].upper() for i in chosen_bad_idx}\n",
    "\n",
    "                    # Include ALL corresponding good samples of those processes\n",
    "                    chosen_good_idx = []\n",
    "                    for proc in chosen_processes:\n",
    "                        chosen_good_idx.extend(proc_good_map.get(proc, []))\n",
    "\n",
    "                    train_idx = list(idx_source) + chosen_bad_idx + chosen_good_idx\n",
    "                    test_idx  = list(idx_target.difference(train_idx))\n",
    "                    chosen_processes = {\n",
    "                        meta.loc[i, \"process\"].upper() for i in chosen_bad_idx\n",
    "                    }\n",
    "                    # Include ALL corresponding good samples of those processes\n",
    "                    chosen_good_idx: List[int] = []\n",
    "                    for proc in chosen_processes:\n",
    "                        chosen_good_idx.extend(proc_good_map.get(proc, []))\n",
    "\n",
    "                    train_idx = list(idx_source) + chosen_bad_idx + chosen_good_idx\n",
    "                    test_idx = list(idx_target.difference(train_idx))\n",
    "\n",
    "                    # Defensive: some k may exhaust target test set\n",
    "                    if (\n",
    "                        len(test_idx) == 0                                   # empty test set\n",
    "                        or len(set(y.loc[train_idx])) < 2                    # training has 1 class\n",
    "                        or len(set(y.loc[test_idx]))   < 2                   # test has 1 class\n",
    "                    ):\n",
    "                        continue  # skip this rep – not informative\n",
    "                    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "                    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "                    pipe = _make_pipeline(BEST_PARAMS, random_state + rep)\n",
    "                    pipe.fit(X_train, y_train)\n",
    "                    y_pred = pipe.predict(X_test)\n",
    "                    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "                    records.append(\n",
    "                        {\n",
    "                            \"source\": source,\n",
    "                            \"target\": target,\n",
    "                            \"k\": k,\n",
    "                            \"rep\": rep,\n",
    "                            \"f1_macro\": f1,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df_all = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Aggregate to summary table with minimal k (n★)\n",
    "    # --------------------------------------------------\n",
    "    summary_rows = []\n",
    "    for (src, tgt), group in df_all.groupby([\"source\", \"target\"]):\n",
    "        medians = group.groupby(\"k\")[\"f1_macro\"].median()\n",
    "        threshold = 0.95 * baseline_f1[tgt]\n",
    "        eligible_ks = sorted([k for k, val in medians.items() if val >= threshold])\n",
    "        n_star = eligible_ks[0] if eligible_ks else None\n",
    "        summary_rows.append({\n",
    "            \"source\": src,\n",
    "            \"target\": tgt,\n",
    "            \"baseline_f1\": baseline_f1[tgt],\n",
    "            \"n_star\": n_star,\n",
    "        })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    return df_all, df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (will take some time)\n",
    "print(\">>> Preparing features (this may take a while)…\")\n",
    "X_features, y_numeric, meta_df = _prepare_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y, y_binary = load_data()\n",
    "# Group X and y by machine without forcing X into a 2D numpy array\n",
    "machines = ['M01', 'M02', 'M03']\n",
    "machine_data = {}\n",
    "\n",
    "for m in machines:\n",
    "    X_m = []\n",
    "    y_m = []\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi.startswith(f\"{m}_\"):\n",
    "            X_m.append(xi)  \n",
    "            y_m.append(0 if yi.endswith(\"_good\") else 1)\n",
    "\n",
    "    # leave X_m as a list of nd‐arrays; convert y_m to numpy if you like\n",
    "    machine_data[m] = (X_m, np.array(y_m))\n",
    "\n",
    "# e.g.:\n",
    "X_M01, y_M01 = machine_data['M01']\n",
    "X_M02, y_M02 = machine_data['M02']\n",
    "X_M03, y_M03 = machine_data['M03']\n",
    "\n",
    "X_M01_tr, y_M01_tr = transform_data(X_M01, y_M01, label_type='binary')\n",
    "X_M02_tr, y_M02_tr = transform_data(X_M02, y_M02, label_type='binary')\n",
    "X_M03_tr, y_M03_tr = transform_data(X_M03, y_M03, label_type='binary')\n",
    "\n",
    "# Create an overview of all the subsets\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(\"\\nMachine-specific breakdown:\")\n",
    "for machine in machines:\n",
    "    X_m, y_m = machine_data[machine]\n",
    "    good_samples = sum(1 for y in y_m if y == 0)\n",
    "    bad_samples = sum(1 for y in y_m if y == 1)\n",
    "    total_samples = len(y_m)\n",
    "    \n",
    "    print(f\"\\n{machine} Dataset:\")\n",
    "    print(f\"  Total samples: {total_samples}\")\n",
    "    print(f\"  Good samples: {good_samples} ({good_samples/total_samples:.2%})\")\n",
    "    print(f\"  Bad samples: {bad_samples} ({bad_samples/total_samples:.2%})\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a subplot for each machine\n",
    "for i, machine in enumerate(machines, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    _, y_m = machine_data[machine]\n",
    "    counts = np.bincount(y_m)\n",
    "    bars = plt.bar(['Good', 'Bad'], counts, color=['green', 'red'])\n",
    "    \n",
    "    # Add count labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{int(height)}',\n",
    "                 ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(f'{machine} Class Distribution')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from grid search\n",
    "best_model\n",
    "\n",
    "# Extract feature importances from the Random Forest classifier\n",
    "feature_importances = best_model.named_steps['rf'].feature_importances_\n",
    "feature_names = trainX_tr.columns\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance and display top 20 features\n",
    "top_features = importance_df.sort_values('Importance', ascending=False).head(20)\n",
    "print(\"Top 20 features selected by the model:\")\n",
    "display(top_features)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features['Feature'][:10], top_features['Importance'][:10])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "# Create path to models folder (going up from src to root, then to models)\n",
    "models_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'models')\n",
    "\n",
    "# Ensure the models directory exists\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save best model in the models folder\n",
    "model_path = os.path.join(models_path, 'EXP1_BINARY_MODEL.pkl')\n",
    "pickle.dump(best_model, open(model_path, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(model_path,'rb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
