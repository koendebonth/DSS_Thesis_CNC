{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "# Get the absolute path to the current notebook directory\n",
    "current_dir = Path().resolve()\n",
    "\n",
    "# Set the project root directory (two levels up from notebooks if in experiments folder)\n",
    "project_root = current_dir.parent.parent\n",
    "\n",
    "# Add the project root to sys.path so Python can find the utils module\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom utilities\n",
    "from utils.load_data import load_data\n",
    "from utils.feature_extraction import transform_data\n",
    "\n",
    "print(\"Dependencies loaded successfully ✅\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Hyper-parameters fixed from Experiment 1 (best RF)\n",
    "# --------------------------------------------------\n",
    "BEST_PARAMS: Dict[str, object] = {\n",
    "    \"rus__sampling_strategy\": 0.2,\n",
    "    \"smote__sampling_strategy\": 0.5,\n",
    "    \"rf__n_estimators\": 200,\n",
    "    \"rf__max_depth\": 15,\n",
    "    \"rf__min_samples_split\": 2,\n",
    "    \"rf__min_samples_leaf\": 1,\n",
    "    \"rf__max_features\": \"sqrt\",\n",
    "}\n",
    "\n",
    "K_GRID: Sequence[int] = (0, 2, 4, 6, 8, 10, 15, 20)\n",
    "N_REPS: int = 20\n",
    "TEST_SIZE: float = 0.3  # fraction of *target* kept for final test\n",
    "RANDOM_STATE: int = 42\n",
    "N_JOBS: int = -1\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "\n",
    "def _parse_label(label: str) -> Tuple[str, str, str]:\n",
    "    \"\"\"Return (machine, process, status) from full label string.\"\"\"\n",
    "    parts = label.split(\"_\")\n",
    "    return parts[0], parts[3], parts[-1]\n",
    "\n",
    "\n",
    "def _build_metadata(labels: Sequence[str]) -> pd.DataFrame:\n",
    "    \"\"\"Convert list of label strings to a metadata DataFrame.\"\"\"\n",
    "    meta = [dict(machine=m, process=p, status=s) for m, p, s in map(_parse_label, labels)]\n",
    "    return pd.DataFrame(meta)\n",
    "\n",
    "\n",
    "def _prepare_features() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "    \"\"\"Load raw data, extract wavelet features and return features, y, meta.\"\"\"\n",
    "    X_raw, y_raw, _ = load_data()  # y_raw are full strings\n",
    "    meta = _build_metadata(y_raw)\n",
    "\n",
    "    # One-shot feature extraction for the full dataset\n",
    "    X_feat, y_num = transform_data(X_raw, y_raw, label_type=\"string\")\n",
    "    # Ensure consistent indices\n",
    "    X_feat.index = meta.index = np.arange(len(meta))\n",
    "    return X_feat, y_num, meta\n",
    "\n",
    "\n",
    "def _make_pipeline(best: Dict[str, object], random_state: int) -> Pipeline:\n",
    "    \"\"\"Instantiate the RF pipeline with fixed hyper-parameters.\"\"\"\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=best[\"rus__sampling_strategy\"], random_state=random_state\n",
    "    )\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=best[\"smote__sampling_strategy\"], random_state=random_state\n",
    "    )\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=best[\"rf__n_estimators\"],\n",
    "        max_depth=best[\"rf__max_depth\"],\n",
    "        min_samples_split=best[\"rf__min_samples_split\"],\n",
    "        min_samples_leaf=best[\"rf__min_samples_leaf\"],\n",
    "        max_features=best[\"rf__max_features\"],\n",
    "        n_jobs=N_JOBS,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return Pipeline([(\"rus\", rus), (\"smote\", smote), (\"rf\", rf)])\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Within-machine reference (for 95 % threshold)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def _within_machine_f1(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    meta: pd.DataFrame,\n",
    "    machine: str,\n",
    "    test_size: float,\n",
    "    random_state: int,\n",
    ") -> float:\n",
    "    \"\"\"Train/test on the same machine; return macro-F1 on hold-out.\"\"\"\n",
    "    idx = meta.index[meta[\"machine\"] == machine]\n",
    "    X_m, y_m = X.loc[idx], y.loc[idx]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_m, y_m, test_size=test_size, stratify=y_m, random_state=random_state\n",
    "    )\n",
    "    pipe = _make_pipeline(BEST_PARAMS, random_state)\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    return f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Core experiment\n",
    "# --------------------------------------------------\n",
    "\n",
    "def run_cross_machine_experiment(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    meta: pd.DataFrame,\n",
    "    k_grid: Sequence[int] = K_GRID,\n",
    "    n_reps: int = N_REPS,\n",
    "    test_size: float = TEST_SIZE,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    baseline_override: float | None = None,      #  NEW\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (df_all, df_summary).\"\"\"\n",
    "    rng = random.Random(random_state)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Baseline(s)\n",
    "    # --------------------------------------------------\n",
    "    if baseline_override is None:\n",
    "        baseline_f1 = {\n",
    "            m: _within_machine_f1(X, y, meta, m, test_size, random_state)\n",
    "            for m in (\"M01\", \"M02\", \"M03\")\n",
    "        }\n",
    "    else:\n",
    "        # same baseline for every target machine\n",
    "        baseline_f1 = {m: baseline_override for m in (\"M01\", \"M02\", \"M03\")}\n",
    "\n",
    "    records: List[Dict[str, object]] = []\n",
    "\n",
    "    for source in (\"M01\", \"M02\", \"M03\"):\n",
    "        for target in (\"M01\", \"M02\", \"M03\"):\n",
    "            if source == target:\n",
    "                continue  # only cross-machine pairs\n",
    "\n",
    "            # Pre-select indices for faster masks\n",
    "            idx_source = meta.index[meta[\"machine\"] == source]\n",
    "            idx_target = meta.index[meta[\"machine\"] == target]\n",
    "            idx_target_bad = idx_target[meta.loc[idx_target, \"status\"] == \"bad\"]\n",
    "            idx_target_good = idx_target[meta.loc[idx_target, \"status\"] == \"good\"]\n",
    "\n",
    "            # Operations mapping (process → good indices)\n",
    "            proc_good_map: Dict[str, List[int]] = defaultdict(list)\n",
    "            for i in idx_target_good:\n",
    "                proc_good_map[meta.loc[i, \"process\"].upper()].append(i)\n",
    "\n",
    "            # List of bad indices per process\n",
    "            proc_bad_map: Dict[str, List[int]] = defaultdict(list)\n",
    "            for i in idx_target_bad:\n",
    "                proc_bad_map[meta.loc[i, \"process\"].upper()].append(i)\n",
    "\n",
    "            for k in k_grid:\n",
    "                for rep in range(n_reps):\n",
    "                    # ── NEW ────────────────────────────────────────────────────────────\n",
    "                    # keep at least one faulty sample for the test set; if impossible,\n",
    "                    # skip this (k, rep) configuration\n",
    "                    if k >= len(idx_target_bad):\n",
    "                        continue\n",
    "\n",
    "                    # draw the k faulty samples that will be added to the training set\n",
    "                    chosen_bad_idx = rng.sample(list(idx_target_bad), k) if k else []\n",
    "                    # ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "                    chosen_processes = {meta.loc[i, \"process\"].upper() for i in chosen_bad_idx}\n",
    "\n",
    "                    # Include ALL corresponding good samples of those processes\n",
    "                    chosen_good_idx = []\n",
    "                    for proc in chosen_processes:\n",
    "                        chosen_good_idx.extend(proc_good_map.get(proc, []))\n",
    "\n",
    "                    train_idx = list(idx_source) + chosen_bad_idx + chosen_good_idx\n",
    "                    test_idx  = list(idx_target.difference(train_idx))\n",
    "                    chosen_processes = {\n",
    "                        meta.loc[i, \"process\"].upper() for i in chosen_bad_idx\n",
    "                    }\n",
    "                    # Include ALL corresponding good samples of those processes\n",
    "                    chosen_good_idx: List[int] = []\n",
    "                    for proc in chosen_processes:\n",
    "                        chosen_good_idx.extend(proc_good_map.get(proc, []))\n",
    "\n",
    "                    train_idx = list(idx_source) + chosen_bad_idx + chosen_good_idx\n",
    "                    test_idx = list(idx_target.difference(train_idx))\n",
    "\n",
    "                    # Defensive: some k may exhaust target test set\n",
    "                    if (\n",
    "                        len(test_idx) == 0                                   # empty test set\n",
    "                        or len(set(y.loc[train_idx])) < 2                    # training has 1 class\n",
    "                        or len(set(y.loc[test_idx]))   < 2                   # test has 1 class\n",
    "                    ):\n",
    "                        continue  # skip this rep – not informative\n",
    "                    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "                    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "                    pipe = _make_pipeline(BEST_PARAMS, random_state + rep)\n",
    "                    pipe.fit(X_train, y_train)\n",
    "                    y_pred = pipe.predict(X_test)\n",
    "                    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "                    records.append(\n",
    "                        {\n",
    "                            \"source\": source,\n",
    "                            \"target\": target,\n",
    "                            \"k\": k,\n",
    "                            \"rep\": rep,\n",
    "                            \"f1_macro\": f1,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df_all = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Aggregate to summary table with minimal k (n★)\n",
    "    # --------------------------------------------------\n",
    "    summary_rows = []\n",
    "    for (src, tgt), group in df_all.groupby([\"source\", \"target\"]):\n",
    "        medians = group.groupby(\"k\")[\"f1_macro\"].median()\n",
    "        threshold = 0.95 * baseline_f1[tgt]\n",
    "        eligible_ks = sorted([k for k, val in medians.items() if val >= threshold])\n",
    "        n_star = eligible_ks[0] if eligible_ks else None\n",
    "        summary_rows.append({\n",
    "            \"source\": src,\n",
    "            \"target\": tgt,\n",
    "            \"baseline_f1\": baseline_f1[tgt],\n",
    "            \"n_star\": n_star,\n",
    "        })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    return df_all, df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (will take some time)\n",
    "print(\">>> Preparing features (this may take a while)…\")\n",
    "X_features, y_numeric, meta_df = _prepare_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cross-machine experiment\n",
    "print(\">>> Running cross-machine experiment …\")\n",
    "df_all, df_summary = run_cross_machine_experiment(\n",
    "    X_features,\n",
    "    y_numeric,\n",
    "    meta_df,\n",
    "    k_grid=tuple(range(0, 21, 1)),\n",
    "    n_reps=N_REPS,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    baseline_override=0.914\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary results\n",
    "print(\"\\n=== Sample-efficiency summary (n★ per Source → Target) ===\")\n",
    "display(df_all)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "df_all.to_csv(\"exp2_full_results.csv\", index=False)\n",
    "df_summary.to_csv(\"exp2_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Plot median F1 per k value for each source-target pair\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (src, tgt), group in df_all.groupby([\"source\", \"target\"]):\n",
    "    medians = group.groupby(\"k\")[\"f1_macro\"].median()\n",
    "    plt.plot(medians.index, medians.values, marker='o', label=f\"{src} → {tgt}\")\n",
    "    \n",
    "    # Add a horizontal line at 95% of the target baseline\n",
    "    threshold = 0.95 * df_summary.loc[(df_summary['source']==src) & \n",
    "                                      (df_summary['target']==tgt), 'baseline_f1'].values[0]\n",
    "    plt.axhline(y=threshold, linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Number of target faulty samples (k)')\n",
    "plt.ylabel('Median Macro F1-score')\n",
    "plt.title('Cross-machine transfer performance by number of target samples')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xticks(range(0, 21, 1))  # Set x-axis ticks with steps of 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_summary.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
