{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = Path(current_dir).parent\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "    print(f\"Added {root_dir} to Python path\")\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "\n",
    "    # Custom utilities\n",
    "    from utils import data_loader_utils\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.model_validation import perform_cross_validation\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    # create and display confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(exclude_processes=None):\n",
    "    \"\"\"\n",
    "    Load data from all machines and processes, with option to exclude specific processes.\n",
    "\n",
    "    Args:\n",
    "        exclude_processes (list, optional): List of process names to exclude from loading.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_data, y_data, y_binary) containing features, full labels, and binary labels\n",
    "    \"\"\"\n",
    "    machines = [\"M01\",\"M02\",\"M03\"]\n",
    "    process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "    labels = [\"good\",\"bad\"]\n",
    "    \n",
    "    # Filter out excluded processes if any\n",
    "    if exclude_processes:\n",
    "        process_names = [p for p in process_names if p not in exclude_processes]\n",
    "    \n",
    "    path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate total number of combinations\n",
    "        total_combinations = len(process_names) * len(machines) * len(labels)\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=total_combinations, desc=\"Loading data\") as pbar:\n",
    "            for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "                data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "                data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "                X_data.extend(data_list)\n",
    "                y_data.extend(data_label)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Samples\": len(X_data)})\n",
    "                \n",
    "        print(f\"Data loaded successfully ✅ - {len(X_data)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "    \n",
    "    # Generate binary labels from full label strings\n",
    "    y_binary = [0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y_data]\n",
    "\n",
    "    return X_data, y_data, y_binary\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "def create_results_df():\n",
    "    \"\"\"\n",
    "    Initialize an empty DataFrame to store experiment results:\n",
    "      – M01_pct, M02_pct, M03_pct: fractions used in the train split\n",
    "      – train_normals, train_anomalies: counts before SMOTE\n",
    "      – train_resampled_normals, train_resampled_anomalies: counts after SMOTE\n",
    "      – test_normals, test_anomalies: counts in the test set\n",
    "      – f1_score: F1 on the test set\n",
    "      – tn, fp, fn, tp: confusion matrix entries\n",
    "    \"\"\"\n",
    "    cols = [\n",
    "        'M01_pct','M02_pct','M03_pct',\n",
    "        'train_normals','train_anomalies',\n",
    "        'train_resampled_normals','train_resampled_anomalies',\n",
    "        'test_normals','test_anomalies',\n",
    "        'f1_score','tn','fp','fn','tp', 'confusion_matrix'\n",
    "    ]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "def record_result(\n",
    "    df,\n",
    "    m01_pct, m02_pct, m03_pct,\n",
    "    trainy, trainy_resampled,\n",
    "    testy,f1, confusion_matrix\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute metrics and append a row to df.\n",
    "    \n",
    "    Args:\n",
    "      df                    – the results DataFrame to append to\n",
    "      m?_pct                – fraction of each machine in the train split\n",
    "      trainy                – original train labels before SMOTE\n",
    "      trainy_resampled      – train labels after SMOTE\n",
    "      testy                 – ground‐truth labels for the test set\n",
    "      y_pred                – predicted labels for the test set\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the new row added.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "    \n",
    "    # Count samples\n",
    "    train_normals  = sum(1 for y in trainy               if y == 0)\n",
    "    train_anomalies = sum(1 for y in trainy               if y == 1)\n",
    "    res_normals     = sum(1 for y in trainy_resampled     if y == 0)\n",
    "    res_anomalies   = sum(1 for y in trainy_resampled     if y == 1)\n",
    "    test_normals    = sum(1 for y in testy                if y == 0)\n",
    "    test_anomalies  = sum(1 for y in testy                if y == 1)\n",
    "    \n",
    "    # Append\n",
    "    df.loc[len(df)] = [\n",
    "        m01_pct, m02_pct, m03_pct,\n",
    "        train_normals, train_anomalies,\n",
    "        res_normals, res_anomalies,\n",
    "        test_normals, test_anomalies,\n",
    "        f1, tn, fp, fn, tp, confusion_matrix\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 == bad | 0 == good\n",
    "X, y, y_binary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_with_adoption(X_data=None, y_data=None, M01=0, M02=0, M03=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model with machine-specific adoption.\n",
    "    \n",
    "    Args:\n",
    "        X_data (list, optional): List of feature data. If None, data will be loaded.\n",
    "        y_data (list, optional): List of labels. If None, data will be loaded.\n",
    "        exclude_process (list): List of processes to exclude from data loading\n",
    "        M01 (float): Percentage (0-1) of M01 data to include in training\n",
    "        M02 (float): Percentage (0-1) of M02 data to include in training\n",
    "        M03 (float): Percentage (0-1) of M03 data to include in training\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, evaluation results, and machine adoption percentages\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'data': X_data, 'label': y_data})\n",
    "    df[['machine', 'month', 'year', 'process', 'sample_id', 'status']] = df['label'].str.split('_', expand=True)\n",
    "\n",
    "    # Initialize empty DataFrames for train and test\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Process each machine separately\n",
    "    for machine, percentage in [('M01', M01), ('M02', M02), ('M03', M03)]:\n",
    "        machine_data = df[df['machine'] == machine]\n",
    "        \n",
    "        if len(machine_data) > 0 and percentage > 0:  # Only process if percentage > 0\n",
    "            # Get the status for stratification\n",
    "            stratify = machine_data['status']\n",
    "            \n",
    "            # Split the data with stratification\n",
    "            if percentage == 1:  # If percentage is 1, use all data for training\n",
    "                train_samples = machine_data\n",
    "                test_samples = pd.DataFrame(columns=machine_data.columns)\n",
    "            else:\n",
    "                train_samples, test_samples = train_test_split(\n",
    "                    machine_data,\n",
    "                    train_size=percentage,\n",
    "                    stratify=stratify,\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            train_dfs.append(train_samples)\n",
    "            test_dfs.append(test_samples)\n",
    "        elif len(machine_data) > 0:  # If percentage is 0, add all to test set\n",
    "            test_dfs.append(machine_data)\n",
    "\n",
    "    # Combine all machine splits\n",
    "    train_df = pd.concat(train_dfs) if train_dfs else pd.DataFrame()\n",
    "    test_df = pd.concat(test_dfs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training set size: {len(train_df)} samples\")\n",
    "        print(f\"Test set size: {len(test_df)} samples\")\n",
    "        print(\"\\nMachine distribution in training set:\")\n",
    "        print(train_df['machine'].value_counts() if not train_df.empty else \"No training data\")\n",
    "        print(\"\\nStatus distribution in training set:\")\n",
    "        print(train_df['status'].value_counts() if not train_df.empty else \"No training data\")\n",
    "        print(\"\\nMachine distribution in test set:\")\n",
    "        print(test_df['machine'].value_counts())\n",
    "        print(\"\\nStatus distribution in test set:\")\n",
    "        print(test_df['status'].value_counts())\n",
    "\n",
    "    # Store status counts before dropping columns\n",
    "    train_status_counts = train_df['status'].value_counts() if not train_df.empty else pd.Series()\n",
    "    test_status_counts = test_df['status'].value_counts()\n",
    "    \n",
    "    train_good = train_status_counts.get('good', 0)\n",
    "    train_bad = train_status_counts.get('bad', 0)\n",
    "    test_good = test_status_counts.get('good', 0)\n",
    "    test_bad = test_status_counts.get('bad', 0)\n",
    "\n",
    "    # Prepare data for training\n",
    "    train_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "    X_train = train_df['data'].tolist()\n",
    "    y_train = train_df['label'].tolist()\n",
    "\n",
    "    test_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "    X_test = test_df['data'].tolist()\n",
    "    y_test = test_df['label'].tolist()\n",
    "\n",
    "    # transform data to features and transform labels to 0 and 1\n",
    "    X_train_features, y_train_labels = transform_data(X_train, y_train, include_metadata=False)\n",
    "    X_test_features, y_test_labels = transform_data(X_test, y_test, include_metadata=False)\n",
    "\n",
    "    #smote oversampling on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_features, y_train_labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"X_train_resampled shape:\", X_train_resampled.shape , \"with smote\")\n",
    "        print(\"X_test_features shape:\", X_test_features.shape , \"without smote\")\n",
    "\n",
    "    # Train Random Forest classifier with optimized hyperparameters\n",
    "    RF = RandomForestClassifier(max_features='log2', \n",
    "                                n_estimators=150,\n",
    "                                max_depth=15,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_samples_split=2,\n",
    "                                random_state=42)\n",
    "\n",
    "    RF.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = RF.predict(X_test_features)\n",
    "    \n",
    "    if verbose:\n",
    "        print(classification_report(y_test_labels, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importances = pd.DataFrame(\n",
    "        RF.feature_importances_,\n",
    "        index=X_train_resampled.columns,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance', ascending=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Top 20 most important features:\")\n",
    "        print(feature_importances.head(5))\n",
    "\n",
    "    # Return the trained model and classification report\n",
    "    report = classification_report(y_test_labels, y_pred, output_dict=True)\n",
    "    \n",
    "    # Create a dictionary with model and evaluation results\n",
    "    result = {\n",
    "        'model': RF,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': confusion_matrix(y_test_labels, y_pred),\n",
    "        'feature_importances': feature_importances,\n",
    "        'machine_adoption': {\n",
    "            'M01': M01,\n",
    "            'M02': M02,\n",
    "            'M03': M03\n",
    "        },\n",
    "        'status_counts': {\n",
    "            'train_good': train_good,\n",
    "            'train_bad': train_bad,\n",
    "            'test_good': test_good,\n",
    "            'test_bad': test_bad\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store results in DataFrame\n",
    "    store_results(result, M01, M02, M03, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX, testX, trainy, testy = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "# trainX = [x_i for x_i, y_i in zip(X, y) \n",
    "#          if y_i.split('_')[0] == 'M01']\n",
    "\n",
    "# # If you also want the corresponding filtered labels:\n",
    "# trainy = [y_i for y_i in y \n",
    "#          if y_i.split('_')[0] == 'M01']\n",
    "\n",
    "# trainy = [0 if label.split('_')[-1] == 'good' else 1 for label in trainy]\n",
    "\n",
    "# testX = [x_i for x_i, y_i in zip(X, y) \n",
    "#          if y_i.split('_')[0] != 'M01']\n",
    "\n",
    "# # If you also want the corresponding filtered labels:\n",
    "# testy = [y_i for y_i in y \n",
    "#          if y_i.split('_')[0] != 'M01']\n",
    "\n",
    "# testy = [0 if label.split('_')[-1] == 'good' else 1 for label in testy]\n",
    "\n",
    "\n",
    "# print(f\"Train set size: {len(trainX)} samples\")\n",
    "# print(f\"Test set size: {len(testX)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = create_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fraction of each machine to go into the TRAIN set\n",
    "machine_train_frac = {\n",
    "    'M01': 1.00,   # 100%\n",
    "    'M02': 0.00,   # 5%\n",
    "    'M03': 0.00    # 0%\n",
    "}\n",
    "\n",
    "trainX, trainy, testX, testy = [], [], [], []\n",
    "\n",
    "for machine, frac in machine_train_frac.items():\n",
    "    # filter out samples for this machine\n",
    "    data = [(x_i, y_i) \n",
    "            for x_i, y_i in zip(X, y) \n",
    "            if y_i.split('_')[0] == machine]\n",
    "    X_m = [d[0] for d in data]\n",
    "    y_m = [0 if d[1].split('_')[-1] == 'good' else 1 for d in data]\n",
    "\n",
    "    # if frac is 0 or 1, no split needed\n",
    "    if frac == 1.0:\n",
    "        trainX += X_m\n",
    "        trainy += y_m\n",
    "    elif frac == 0.0:\n",
    "        testX += X_m\n",
    "        testy += y_m\n",
    "    else:\n",
    "        # do a stratified split to keep minority ratio\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_m, y_m,\n",
    "            train_size=frac,\n",
    "            test_size=1 - frac,\n",
    "            stratify=y_m,\n",
    "            random_state=42\n",
    "        )\n",
    "        trainX += X_tr\n",
    "        trainy += y_tr\n",
    "        testX += X_te\n",
    "        testy += y_te\n",
    "\n",
    "# check minority class distribution\n",
    "print(f\"Train size: {len(trainX)} samples\")\n",
    "print(f\" Test minority ratio: {np.mean(trainy):.3f}\")\n",
    "print(f\"Test size:  {len(testX)} samples\")\n",
    "print(f\" Test minority ratio: {np.mean(testy):.3f}\")\n",
    "\n",
    "trainX_tr, trainy_tr = transform_data(trainX,trainy, label_type='binary')\n",
    "testX_tr, testy_tr = transform_data(testX, testy, label_type='binary')\n",
    "\n",
    "#smote oversampling on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "trainX_tr_resampled, trainy_tr_resampled = smote.fit_resample(trainX_tr, trainy_tr)\n",
    "\n",
    "test_y = testy_tr\n",
    "test_y[test_y == 1] = -1\n",
    "test_y[test_y == 0] = 1\n",
    "\n",
    "model_X = trainX_tr\n",
    "model_y = trainy_tr\n",
    "\n",
    "\n",
    "# define outlier detection model\n",
    "model = OneClassSVM(gamma='scale', nu=0.01)\n",
    "# fit on majority class\n",
    "trainX_tr_zero = model_X[model_y==0]\n",
    "model.fit(trainX_tr_zero)\n",
    "# detect outliers in the test set\n",
    "yhat = model.predict(testX_tr)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "# calculate score\n",
    "score = f1_score(test_y, yhat, pos_label=-1, average='binary')\n",
    "print('F1 Score: %.3f' % score)\n",
    "\n",
    "cm = confusion_matrix(test_y, yhat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Anomaly', 'Normal'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "record_result(result_df, machine_train_frac['M01'], machine_train_frac['M02'], machine_train_frac['M03'], trainy, trainX_tr_resampled, testy, score, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
