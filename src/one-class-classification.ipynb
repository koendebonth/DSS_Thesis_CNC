{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = Path(current_dir).parent\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "    print(f\"Added {root_dir} to Python path\")\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "\n",
    "    # Custom utilities\n",
    "    from utils import data_loader_utils\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.model_validation import perform_cross_validation\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(exclude_processes=None):\n",
    "    \"\"\"\n",
    "    Load data from all machines and processes, with option to exclude specific processes.\n",
    "\n",
    "    Args:\n",
    "        exclude_processes (list, optional): List of process names to exclude from loading.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_data, y_data, y_binary) containing features, full labels, and binary labels\n",
    "    \"\"\"\n",
    "    machines = [\"M01\",\"M02\",\"M03\"]\n",
    "    process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "    labels = [\"good\",\"bad\"]\n",
    "    \n",
    "    # Filter out excluded processes if any\n",
    "    if exclude_processes:\n",
    "        process_names = [p for p in process_names if p not in exclude_processes]\n",
    "    \n",
    "    path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate total number of combinations\n",
    "        total_combinations = len(process_names) * len(machines) * len(labels)\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=total_combinations, desc=\"Loading data\") as pbar:\n",
    "            for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "                data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "                data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "                X_data.extend(data_list)\n",
    "                y_data.extend(data_label)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Samples\": len(X_data)})\n",
    "                \n",
    "        print(f\"Data loaded successfully ✅ - {len(X_data)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "    \n",
    "    # Generate binary labels from full label strings\n",
    "    y_binary = [0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y_data]\n",
    "\n",
    "    return X_data, y_data, y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 == bad | 0 == good\n",
    "X, y, y_binary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainX_tr, trainy_tr = transform_data(trainX,trainy, label_type='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_tr, testy_tr = transform_data(testX, testy, label_type='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of normal (0) and anomaly (1) samples in training and test sets\n",
    "train_normal_count = (trainy_tr == 0).sum()\n",
    "train_anomaly_count = (trainy_tr == 1).sum()\n",
    "test_normal_count = (testy_tr == 0).sum()\n",
    "test_anomaly_count = (testy_tr == 1).sum()\n",
    "\n",
    "print(f\"Training set: {train_normal_count} normal samples, {train_anomaly_count} anomaly samples\")\n",
    "print(f\"Test set: {test_normal_count} normal samples, {test_anomaly_count} anomaly samples\")\n",
    "# print(f\"Training set distribution: {train_normal_count/(train_normal_count+train_anomaly_count)*100:.2f}% normal, {train_anomaly_count/(train_normal_count+train_anomaly_count)*100:.2f}% anomaly\")\n",
    "# print(f\"Test set distribution: {test_normal_count/(test_normal_count+test_anomaly_count)*100:.2f}% normal, {test_anomaly_count/(test_normal_count+test_anomaly_count)*100:.2f}% anomaly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outlier detection model\n",
    "model = OneClassSVM(gamma='scale', nu=0.01)\n",
    "# fit on majority class\n",
    "trainX_tr_zero = trainX_tr[trainy_tr==0]\n",
    "model.fit(trainX_tr_zero)\n",
    "# detect outliers in the test set\n",
    "yhat = model.predict(testX_tr)\n",
    "# mark inliers 1, outliers -1\n",
    "testy_tr[testy_tr == 1] = -1\n",
    "testy_tr[testy_tr == 0] = 1\n",
    "# calculate score\n",
    "score = f1_score(testy_tr, yhat, pos_label=-1, average='binary')\n",
    "print('F1 Score: %.3f' % score)\n",
    "\n",
    "# create and display confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(testy_tr, yhat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Anomaly'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
