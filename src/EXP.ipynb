{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Fine tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = Path(current_dir).parent\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "    print(f\"Added {root_dir} to Python path\")\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "\n",
    "    # Custom utilities\n",
    "    from utils import data_loader_utils\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.model_validation import perform_cross_validation\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(exclude_processes=None):\n",
    "    \"\"\"\n",
    "    Load data from all machines and processes, with option to exclude specific processes.\n",
    "    \n",
    "    Args:\n",
    "        exclude_processes (list, optional): List of process names to exclude from loading.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_data, y_data) containing features and labels\n",
    "    \"\"\"\n",
    "    machines = [\"M01\",\"M02\",\"M03\"]\n",
    "    process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "    labels = [\"good\",\"bad\"]\n",
    "    \n",
    "    # Filter out excluded processes if any\n",
    "    if exclude_processes:\n",
    "        process_names = [p for p in process_names if p not in exclude_processes]\n",
    "    \n",
    "    path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate total number of combinations\n",
    "        total_combinations = len(process_names) * len(machines) * len(labels)\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=total_combinations, desc=\"Loading data\") as pbar:\n",
    "            for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "                data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "                data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "                X_data.extend(data_list)\n",
    "                y_data.extend(data_label)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Samples\": len(X_data)})\n",
    "                \n",
    "        print(f\"Data loaded successfully ✅ - {len(X_data)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_with_adoption(exclude_process=[], M01=0, M02=0, M03=0):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model with machine-specific adoption.\n",
    "    \n",
    "    Args:\n",
    "        exclude_process (list): List of processes to exclude\n",
    "        M01 (float): Percentage (0-1) of M01 data to include in training\n",
    "        M02 (float): Percentage (0-1) of M02 data to include in training\n",
    "        M03 (float): Percentage (0-1) of M03 data to include in training\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df) containing the split data\n",
    "    \"\"\"\n",
    "    X_data, y_data = load_data(exclude_process)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'data': X_data, 'label': y_data})\n",
    "    df[['machine', 'month', 'year', 'process', 'sample_id', 'status']] = df['label'].str.split('_', expand=True)\n",
    "\n",
    "    # Initialize empty DataFrames for train and test\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Process each machine separately\n",
    "    for machine, percentage in [('M01', M01), ('M02', M02), ('M03', M03)]:\n",
    "        machine_data = df[df['machine'] == machine]\n",
    "        \n",
    "        if len(machine_data) > 0 and percentage > 0:  # Only process if percentage > 0\n",
    "            # Get the status for stratification\n",
    "            stratify = machine_data['status']\n",
    "            \n",
    "            # Split the data with stratification\n",
    "            train_samples, test_samples = train_test_split(\n",
    "                machine_data,\n",
    "                train_size=percentage,\n",
    "                stratify=stratify,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            train_dfs.append(train_samples)\n",
    "            test_dfs.append(test_samples)\n",
    "        elif len(machine_data) > 0:  # If percentage is 0, add all to test set\n",
    "            test_dfs.append(machine_data)\n",
    "\n",
    "    # Combine all machine splits\n",
    "    train_df = pd.concat(train_dfs) if train_dfs else pd.DataFrame()\n",
    "    test_df = pd.concat(test_dfs)\n",
    "\n",
    "    print(f\"Training set size: {len(train_df)} samples\")\n",
    "    print(f\"Test set size: {len(test_df)} samples\")\n",
    "    print(\"\\nMachine distribution in training set:\")\n",
    "    print(train_df['machine'].value_counts() if not train_df.empty else \"No training data\")\n",
    "    print(\"\\nStatus distribution in training set:\")\n",
    "    print(train_df['status'].value_counts() if not train_df.empty else \"No training data\")\n",
    "    print(\"\\nMachine distribution in test set:\")\n",
    "    print(test_df['machine'].value_counts())\n",
    "    print(\"\\nStatus distribution in test set:\")\n",
    "    print(test_df['status'].value_counts())\n",
    "\n",
    "    # Prepare data for training\n",
    "    train_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "    X_train = train_df['data'].tolist()\n",
    "    y_train = train_df['label'].tolist()\n",
    "\n",
    "    test_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "\n",
    "    X_test = test_df['data'].tolist()\n",
    "    y_test = test_df['label'].tolist()\n",
    "\n",
    "    # transform data to features and transform labels to 0 and 1\n",
    "\n",
    "    X_train_features, y_train_labels = transform_data(X_train, y_train, include_metadata=False)\n",
    "\n",
    "    X_test_features, y_test_labels = transform_data(X_test, y_test, include_metadata=False)\n",
    "\n",
    "    #smote oversampling on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    X_train_features, y_train_labels = smote.fit_resample(X_train_features, y_train_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return X_train_features, y_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "X_data, y_data = load_data()  # Load all processes\n",
    "# X_data, y_data = load_data(exclude_processes=[\"OP00\"])  # Exclude specific processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use 50% of M01 data, 30% of M02 data, and 0% of M03 data in training\n",
    "train_df, test_df = train_rf_with_adoption(M01=0.5, M02=0.3, M03=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
