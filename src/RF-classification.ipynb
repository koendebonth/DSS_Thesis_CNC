{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = Path(current_dir).parent\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "    print(f\"Added {root_dir} to Python path\")\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "\n",
    "    # Custom utilities\n",
    "    from utils import data_loader_utils\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.model_validation import perform_cross_validation\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    # create and display confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(exclude_processes=None):\n",
    "    \"\"\"\n",
    "    Load data from all machines and processes, with option to exclude specific processes.\n",
    "\n",
    "    Args:\n",
    "        exclude_processes (list, optional): List of process names to exclude from loading.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_data, y_data, y_binary) containing features, full labels, and binary labels\n",
    "    \"\"\"\n",
    "    machines = [\"M01\",\"M02\",\"M03\"]\n",
    "    process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "    labels = [\"good\",\"bad\"]\n",
    "    \n",
    "    # Filter out excluded processes if any\n",
    "    if exclude_processes:\n",
    "        process_names = [p for p in process_names if p not in exclude_processes]\n",
    "    \n",
    "    path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate total number of combinations\n",
    "        total_combinations = len(process_names) * len(machines) * len(labels)\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=total_combinations, desc=\"Loading data\") as pbar:\n",
    "            for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "                data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "                data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "                X_data.extend(data_list)\n",
    "                y_data.extend(data_label)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Samples\": len(X_data)})\n",
    "                \n",
    "        print(f\"Data loaded successfully ✅ - {len(X_data)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "    \n",
    "    # Generate binary labels from full label strings\n",
    "    y_binary = [0 if label_str.split(\"_\")[-1] == \"good\" else 1 for label_str in y_data]\n",
    "\n",
    "    return X_data, y_data, y_binary\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "def create_results_df():\n",
    "    \"\"\"\n",
    "    Initialize an empty DataFrame to store experiment results:\n",
    "      – M01_pct, M02_pct, M03_pct: fractions used in the train split\n",
    "      – train_normals, train_anomalies: counts before SMOTE\n",
    "      – train_resampled_normals, train_resampled_anomalies: counts after SMOTE\n",
    "      – test_normals, test_anomalies: counts in the test set\n",
    "      – f1_score: F1 on the test set\n",
    "      – tn, fp, fn, tp: confusion matrix entries\n",
    "    \"\"\"\n",
    "    cols = [\n",
    "        'M01_pct','M02_pct','M03_pct',\n",
    "        'train_normals','train_anomalies',\n",
    "        'train_resampled_normals','train_resampled_anomalies',\n",
    "        'test_normals','test_anomalies',\n",
    "        'f1_score','tn','fp','fn','tp', 'confusion_matrix', 'experiment_id'\n",
    "    ]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "def record_result(\n",
    "    df,\n",
    "    m01_pct, m02_pct, m03_pct,\n",
    "    trainy, trainy_resampled,\n",
    "    testy, f1, confusion_matrix,\n",
    "    experiment_id=None\n",
    "):\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "    train_normals  = sum(1 for y in trainy           if y == 0)\n",
    "    train_anomalies = sum(1 for y in trainy           if y == 1)\n",
    "    res_normals     = sum(1 for y in trainy_resampled if y == 0)\n",
    "    res_anomalies   = sum(1 for y in trainy_resampled if y == 1)\n",
    "    test_normals    = sum(1 for y in testy            if y == 0)\n",
    "    test_anomalies  = sum(1 for y in testy            if y == 1)\n",
    "\n",
    "    df.loc[len(df)] = [\n",
    "        m01_pct, m02_pct, m03_pct,\n",
    "        train_normals, train_anomalies,\n",
    "        res_normals, res_anomalies,\n",
    "        test_normals, test_anomalies,\n",
    "        f1, tn, fp, fn, tp,\n",
    "        confusion_matrix,\n",
    "        experiment_id\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 == good | 1 == bad |\n",
    "X, y, y_binary = load_data()\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "print(f\"Train set size: {len(trainX)} samples\")\n",
    "print(f\"Test set size: {len(testX)} samples\")\n",
    "\n",
    "# transform and resample\n",
    "trainX_tr, trainy_tr = transform_data(trainX, trainy, label_type='binary')\n",
    "\n",
    "smote = SMOTE(k_neighbors=5, random_state=42)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "trainX_tr, trainy_tr = rus.fit_resample(trainX_tr, trainy_tr)\n",
    "trainX_tr_resampled, trainy_tr_resampled = smote.fit_resample(trainX_tr, trainy_tr)\n",
    "testX_tr, testy_tr = transform_data(testX, testy, label_type='binary')\n",
    "\n",
    "    # Train Random Forest classifier with optimized hyperparameters\n",
    "RF = RandomForestClassifier(max_features='log2', \n",
    "                            n_estimators=150,\n",
    "                            max_depth=15,\n",
    "                            min_samples_leaf=1,\n",
    "                            min_samples_split=2,\n",
    "                            random_state=42)\n",
    "\n",
    "RF.fit(trainX_tr_resampled, trainy_tr_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "yhat = RF.predict(testX_tr)\n",
    "score = f1_score(testy_tr, yhat, pos_label=1, average='binary')\n",
    "cm = confusion_matrix(testy_tr, yhat)\n",
    "# record results\n",
    "record_result(result_df_RF, 0, 0, 0, trainy, trainy_tr_resampled, testy, score, cm, experiment_id='exp0_no_machine_adoption_random_split')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_RF = create_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(m01, m02, m03):\n",
    "    '''Run one-class SVM experiment with specified machine fractions.'''   \n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "    machine_train_frac = {'M01': m01, 'M02': m02, 'M03': m03}\n",
    "    trainX, trainy, testX, testy = [], [], [], []\n",
    "    \n",
    "    for machine, frac in machine_train_frac.items():\n",
    "        # filter samples for this machine\n",
    "        data = [(x_i, y_i) for x_i, y_i in zip(Xtrain, ytrain)]\n",
    "        X_m = [d[0] for d in data if d[1].split('_')[0] == machine]\n",
    "        y_m = [0 if d[1].split('_')[-1] == 'good' else 1 for d in data if d[1].split('_')[0] == machine]\n",
    "        if frac == 1.0:\n",
    "            trainX.extend(X_m); trainy.extend(y_m)\n",
    "        elif frac == 0.0:\n",
    "            testX.extend(X_m); testy.extend(y_m)\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_m, y_m, train_size=frac, stratify=y_m, random_state=42\n",
    "            )\n",
    "            trainX.extend(X_tr); trainy.extend(y_tr)\n",
    "            testX.extend(X_te); testy.extend(y_te)\n",
    "\n",
    "    # transform and resample\n",
    "    trainX_tr, trainy_tr = transform_data(trainX, trainy, label_type='binary')\n",
    "    testX_tr, testy_tr = transform_data(Xtest, ytest, label_type='binary')\n",
    "    # Print class distribution before resampling\n",
    "    print(f\"Class distribution before resampling: {pd.Series(trainy_tr).value_counts()}\")\n",
    "    \n",
    "    # Apply RandomUnderSampler before SMOTE for better balance\n",
    "    # This helps reduce the majority class before applying SMOTE\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    trainX_tr, trainy_tr = rus.fit_resample(trainX_tr, trainy_tr)\n",
    "    \n",
    "    # Print class distribution after undersampling\n",
    "    print(f\"Class distribution after undersampling: {pd.Series(trainy_tr).value_counts()}\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    trainX_tr_resampled, trainy_tr_resampled = smote.fit_resample(trainX_tr, trainy_tr)\n",
    "    \n",
    "\n",
    "        # Train Random Forest classifier with optimized hyperparameters\n",
    "    RF = RandomForestClassifier(max_features='log2', \n",
    "                                n_estimators=150,\n",
    "                                max_depth=15,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_samples_split=2,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    RF.fit(trainX_tr_resampled, trainy_tr_resampled)\n",
    "    # Evaluate the model\n",
    "    yhat = RF.predict(testX_tr)\n",
    "    score = f1_score(testy_tr, yhat, pos_label=1, average='binary')\n",
    "    cm = confusion_matrix(testy_tr, yhat)\n",
    "    # record results\n",
    "    record_result(result_df_RF, m01, m02, m03, trainy, trainy_tr_resampled, testy, score, cm)\n",
    "    \n",
    "# e.g. experiment 1: vary M02\n",
    "for m02 in np.arange(0, 0.51, 0.05):\n",
    "    run_one_class_experiment(1.0, m02, 0.0)\n",
    "    result_df_RF.loc[result_df_RF.index[-1], 'experiment_id'] = 'exp1_vary_M02'\n",
    "\n",
    "# e.g. experiment 2: vary M03\n",
    "for m03 in np.arange(0.05, 0.51, 0.05):\n",
    "    run_one_class_experiment(1.0, 0.0, m03)\n",
    "    result_df_RF.loc[result_df_RF.index[-1], 'experiment_id'] = 'exp2_vary_M03'\n",
    "\n",
    "# e.g. experiment 3: vary M02 and M03 together\n",
    "for frac in np.arange(0.05, 0.51, 0.05):\n",
    "    run_one_class_experiment(1.0, frac, frac)\n",
    "    result_df_RF.loc[result_df_RF.index[-1], 'experiment_id'] = 'exp3_vary_both'\n",
    "\n",
    "# Display the compiled results\n",
    "result_df_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.dirname(os.getcwd()), 'export')\n",
    "result_df_RF.to_csv(path + '/results/result_df_RF_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for machine, frac in machine_train_frac.items():\n",
    "        # filter samples for this machine\n",
    "        data = [(x_i, y_i) for x_i, y_i in zip(Xtrain, ytrain)]\n",
    "        X_m = [d[0] for d in data if d[1].split('_')[0] == machine]\n",
    "        y_m = [0 if d[1].split('_')[-1] == 'good' else 1 for d in data if d[1].split('_')[0] == machine]\n",
    "        if frac == 1.0:\n",
    "            trainX.extend(X_m); trainy.extend(y_m)\n",
    "        elif frac == 0.0:\n",
    "            testX.extend(X_m); testy.extend(y_m)\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_m, y_m, train_size=frac, stratify=y_m, random_state=42\n",
    "            )\n",
    "            trainX.extend(X_tr); trainy.extend(y_tr)\n",
    "            testX.extend(X_te); testy.extend(y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
