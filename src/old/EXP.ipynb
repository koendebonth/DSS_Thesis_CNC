{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Fine tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import depencies\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Set the root directory to the parent of the current directory\n",
    "    root_dir = Path(current_dir).parent\n",
    "\n",
    "    # Add the root directory to sys.path so Python can find the utils module\n",
    "    sys.path.append(str(root_dir))\n",
    "    print(f\"Added {root_dir} to Python path\")\n",
    "\n",
    "    # Standard libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import itertools\n",
    "    import h5py\n",
    "\n",
    "    # Data processing and visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import signal, stats\n",
    "    import pywt\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Machine learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "\n",
    "    # Custom utilities\n",
    "    from utils import data_loader_utils\n",
    "    from utils.feature_extraction import transform_data\n",
    "    from utils.model_validation import perform_cross_validation\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    print(\"Dependencies loaded successfully ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dependencies: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(exclude_processes=None):\n",
    "    \"\"\"\n",
    "    Load data from all machines and processes, with option to exclude specific processes.\n",
    "    \n",
    "    Args:\n",
    "        exclude_processes (list, optional): List of process names to exclude from loading.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_data, y_data) containing features and labels\n",
    "    \"\"\"\n",
    "    machines = [\"M01\",\"M02\",\"M03\"]\n",
    "    process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "    labels = [\"good\",\"bad\"]\n",
    "    \n",
    "    # Filter out excluded processes if any\n",
    "    if exclude_processes:\n",
    "        process_names = [p for p in process_names if p not in exclude_processes]\n",
    "    \n",
    "    path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate total number of combinations\n",
    "        total_combinations = len(process_names) * len(machines) * len(labels)\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=total_combinations, desc=\"Loading data\") as pbar:\n",
    "            for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "                data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "                data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "                X_data.extend(data_list)\n",
    "                y_data.extend(data_label)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Samples\": len(X_data)})\n",
    "                \n",
    "        print(f\"Data loaded successfully ✅ - {len(X_data)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_good_class_to_minimum(X_data, y_data, machines_to_reduce=['M01']):\n",
    "    \"\"\"\n",
    "    Undersample the 'good' class from specified machine(s) so that each process\n",
    "    for each machine ends up at its per‐process minimum.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # build DataFrame and split out metadata\n",
    "    df = pd.DataFrame({'data': X_data, 'label': y_data})\n",
    "    df[['machine','month','year','process','sample_id','status']] = \\\n",
    "        df['label'].str.split('_', expand=True)\n",
    "\n",
    "    good = df[df['status']=='good']\n",
    "    bad  = df[df['status']=='bad']\n",
    "\n",
    "    # allow passing a single string\n",
    "    if isinstance(machines_to_reduce, str):\n",
    "        machines_to_reduce = [machines_to_reduce]\n",
    "\n",
    "    reduced_good = pd.DataFrame()\n",
    "    for m in machines_to_reduce:\n",
    "        sub = good[good['machine']==m]\n",
    "        if sub.empty: \n",
    "            continue\n",
    "        counts = sub['process'].value_counts()\n",
    "        mcount = counts.min()\n",
    "        # sample each process down to the min\n",
    "        for proc, _ in counts.items():\n",
    "            proc_samples = sub[sub['process']==proc]\n",
    "            sampled = proc_samples.sample(n=mcount, random_state=42)\n",
    "            reduced_good = pd.concat([reduced_good, sampled], axis=0)\n",
    "\n",
    "    # keep all good samples from other machines plus all bad samples\n",
    "    other_good = good[~good['machine'].isin(machines_to_reduce)]\n",
    "    final_df  = pd.concat([reduced_good, other_good, bad], axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Original: {len(df)} rows → Reduced: {len(final_df)} rows\")\n",
    "    return final_df['data'].tolist(), final_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results or create a new DataFrame to store results\n",
    "import os\n",
    "\n",
    "results_file = 'results_df.csv'\n",
    "if os.path.exists(results_file):\n",
    "    # Load existing results\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    print(f\"Loaded existing results with {len(results_df)} entries\")\n",
    "else:\n",
    "    # Create a new DataFrame\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'timestamp', 'M01_adoption', 'M02_adoption', 'M03_adoption',\n",
    "        'accuracy', 'precision', 'recall', 'f1_score',\n",
    "        'train_samples', 'test_samples',\n",
    "        'good_train', 'bad_train', 'good_test', 'bad_test'\n",
    "    ])\n",
    "    print(\"Created new results DataFrame\")\n",
    "\n",
    "def store_results(result, M01, M02, M03, y_train_labels, y_test_labels):\n",
    "    \"\"\"\n",
    "    Store model results in the results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Dictionary containing model results\n",
    "        M01 (float): M01 adoption percentage\n",
    "        M02 (float): M02 adoption percentage\n",
    "        M03 (float): M03 adoption percentage\n",
    "        y_train_labels (array-like): Training labels\n",
    "        y_test_labels (array-like): Test labels\n",
    "    \"\"\"\n",
    "    global results_df\n",
    "\n",
    "    results_file = '../export/results_df.csv'\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        # Load existing results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "        print(f\"Loaded existing results with {len(results_df)} entries\")\n",
    "    else:\n",
    "        # Create a new DataFrame\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'timestamp', 'M01_adoption', 'M02_adoption', 'M03_adoption',\n",
    "            'accuracy', 'precision', 'recall', 'f1_score',\n",
    "            'train_samples', 'test_samples',\n",
    "            'good_train', 'bad_train', 'good_test', 'bad_test'\n",
    "        ])\n",
    "        print(\"Created new results DataFrame\")\n",
    "    \n",
    "    # Get classification report metrics\n",
    "    report = result['classification_report']\n",
    "    \n",
    "    # Calculate class distributions from the labels\n",
    "    train_good = np.sum(y_train_labels == 0)\n",
    "    train_bad = np.sum(y_train_labels == 1)\n",
    "    test_good = np.sum(y_test_labels == 0)\n",
    "    test_bad = np.sum(y_test_labels == 1)\n",
    "    \n",
    "    # Create new row with timestamp\n",
    "    new_row = {\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'M01_adoption': M01,\n",
    "        'M02_adoption': M02,\n",
    "        'M03_adoption': M03,\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1_score': report['weighted avg']['f1-score'],\n",
    "        'train_samples': len(y_train_labels),\n",
    "        'test_samples': len(y_test_labels),\n",
    "        'good_train': train_good,\n",
    "        'bad_train': train_bad,\n",
    "        'good_test': test_good,\n",
    "        'bad_test': test_bad\n",
    "    }\n",
    "    \n",
    "    # Append to results DataFrame\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Sort by f1_score in descending order\n",
    "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "    results_df.to_csv('../export/results_df.csv', index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_with_adoption(X_data=None, y_data=None, M01=0, M02=0, M03=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model with machine-specific adoption.\n",
    "    \n",
    "    Args:\n",
    "        X_data (list, optional): List of feature data. If None, data will be loaded.\n",
    "        y_data (list, optional): List of labels. If None, data will be loaded.\n",
    "        exclude_process (list): List of processes to exclude from data loading\n",
    "        M01 (float): Percentage (0-1) of M01 data to include in training\n",
    "        M02 (float): Percentage (0-1) of M02 data to include in training\n",
    "        M03 (float): Percentage (0-1) of M03 data to include in training\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, evaluation results, and machine adoption percentages\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'data': X_data, 'label': y_data})\n",
    "    df[['machine', 'month', 'year', 'process', 'sample_id', 'status']] = df['label'].str.split('_', expand=True)\n",
    "\n",
    "    # Initialize empty DataFrames for train and test\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Process each machine separately\n",
    "    for machine, percentage in [('M01', M01), ('M02', M02), ('M03', M03)]:\n",
    "        machine_data = df[df['machine'] == machine]\n",
    "        \n",
    "        if len(machine_data) > 0 and percentage > 0:  # Only process if percentage > 0\n",
    "            # Get the status for stratification\n",
    "            stratify = machine_data['status']\n",
    "            \n",
    "            # Split the data with stratification\n",
    "            if percentage == 1:  # If percentage is 1, use all data for training\n",
    "                train_samples = machine_data\n",
    "                test_samples = pd.DataFrame(columns=machine_data.columns)\n",
    "            else:\n",
    "                train_samples, test_samples = train_test_split(\n",
    "                    machine_data,\n",
    "                    train_size=percentage,\n",
    "                    stratify=stratify,\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            train_dfs.append(train_samples)\n",
    "            test_dfs.append(test_samples)\n",
    "        elif len(machine_data) > 0:  # If percentage is 0, add all to test set\n",
    "            test_dfs.append(machine_data)\n",
    "\n",
    "    # Combine all machine splits\n",
    "    train_df = pd.concat(train_dfs) if train_dfs else pd.DataFrame()\n",
    "    test_df = pd.concat(test_dfs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training set size: {len(train_df)} samples\")\n",
    "        print(f\"Test set size: {len(test_df)} samples\")\n",
    "        print(\"\\nMachine distribution in training set:\")\n",
    "        print(train_df['machine'].value_counts() if not train_df.empty else \"No training data\")\n",
    "        print(\"\\nStatus distribution in training set:\")\n",
    "        print(train_df['status'].value_counts() if not train_df.empty else \"No training data\")\n",
    "        print(\"\\nMachine distribution in test set:\")\n",
    "        print(test_df['machine'].value_counts())\n",
    "        print(\"\\nStatus distribution in test set:\")\n",
    "        print(test_df['status'].value_counts())\n",
    "\n",
    "    # Store status counts before dropping columns\n",
    "    train_status_counts = train_df['status'].value_counts() if not train_df.empty else pd.Series()\n",
    "    test_status_counts = test_df['status'].value_counts()\n",
    "    \n",
    "    train_good = train_status_counts.get('good', 0)\n",
    "    train_bad = train_status_counts.get('bad', 0)\n",
    "    test_good = test_status_counts.get('good', 0)\n",
    "    test_bad = test_status_counts.get('bad', 0)\n",
    "\n",
    "    # Prepare data for training\n",
    "    train_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "    X_train = train_df['data'].tolist()\n",
    "    y_train = train_df['label'].tolist()\n",
    "\n",
    "    test_df.drop(columns=['machine', 'month', 'year', 'process', 'sample_id', 'status'], inplace=True)\n",
    "    X_test = test_df['data'].tolist()\n",
    "    y_test = test_df['label'].tolist()\n",
    "\n",
    "    # transform data to features and transform labels to 0 and 1\n",
    "    X_train_features, y_train_labels = transform_data(X_train, y_train, include_metadata=False)\n",
    "    X_test_features, y_test_labels = transform_data(X_test, y_test, include_metadata=False)\n",
    "\n",
    "    #smote oversampling on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_features, y_train_labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"X_train_resampled shape:\", X_train_resampled.shape , \"with smote\")\n",
    "        print(\"X_test_features shape:\", X_test_features.shape , \"without smote\")\n",
    "\n",
    "    # Train Random Forest classifier with optimized hyperparameters\n",
    "    RF = RandomForestClassifier(max_features='log2', \n",
    "                                n_estimators=150,\n",
    "                                max_depth=15,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_samples_split=2,\n",
    "                                random_state=42)\n",
    "\n",
    "    RF.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = RF.predict(X_test_features)\n",
    "    \n",
    "    if verbose:\n",
    "        print(classification_report(y_test_labels, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "    # Feature importance\n",
    "    feature_importances = pd.DataFrame(\n",
    "        RF.feature_importances_,\n",
    "        index=X_train_resampled.columns,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance', ascending=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Top 20 most important features:\")\n",
    "        print(feature_importances.head(5))\n",
    "\n",
    "    # Return the trained model and classification report\n",
    "    report = classification_report(y_test_labels, y_pred, output_dict=True)\n",
    "    \n",
    "    # Create a dictionary with model and evaluation results\n",
    "    result = {\n",
    "        'model': RF,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': confusion_matrix(y_test_labels, y_pred),\n",
    "        'feature_importances': feature_importances,\n",
    "        'machine_adoption': {\n",
    "            'M01': M01,\n",
    "            'M02': M02,\n",
    "            'M03': M03\n",
    "        },\n",
    "        'status_counts': {\n",
    "            'train_good': train_good,\n",
    "            'train_bad': train_bad,\n",
    "            'test_good': test_good,\n",
    "            'test_bad': test_bad\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store results in DataFrame\n",
    "    store_results(result, M01, M02, M03, y_train_labels, y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "X_data, y_data = load_data()  # Load all processes\n",
    "# X_data, y_data = load_data(exclude_processes=[\"OP00\"])  # Exclude specific processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce good samples from M01 by keeping only 30%\n",
    "Xr, yr = undersample_good_class_to_minimum(X_data, y_data, machines_to_reduce=['M01','M02'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use 50% of M01 data, 30% of M02 data, and 0% of M03 data in training\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.0,  M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.05, M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.1,  M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.15, M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.2,  M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.25, M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.3,  M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.35, M03=0.0)\n",
    "train_rf_with_adoption(X_data=Xr, y_data=yr, M01=1.0, M02=0.4,  M03=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bekijk alle resultaten met timestamps\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv('../export/results_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EXP_improved import run_holdout_experiment_preloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = run_holdout_experiment_preloaded(\n",
    "    X_data=X_data,\n",
    "    y_data=y_data,\n",
    "    M01=1.0,\n",
    "    M02=0.0,\n",
    "    M03=0.0,\n",
    "    machines_to_reduce=[\"M01\",\"M02\"],\n",
    "    random_state=42,\n",
    "    check_split=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
