{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS Thesis - Koen de Bonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:57:31.567105Z",
     "start_time": "2025-03-25T13:57:31.562498Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set the root directory to the parent of the current directory\n",
    "root_dir = Path(current_dir).parent\n",
    "\n",
    "# Add the root directory to sys.path so Python can find the utils module\n",
    "sys.path.append(str(root_dir))\n",
    "print(f\"Added {root_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:57:31.533907Z",
     "start_time": "2025-03-25T13:57:31.443496Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import data_loader_utils\n",
    "from utils.feature_extraction import transform_data, prepare_train_test_data\n",
    "import itertools \n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pywt\n",
    "import numpy as np\n",
    "from scipy import signal,stats\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "machines = [\"M01\",\"M02\",\"M03\"]\n",
    "\n",
    "# total list of operations\n",
    "# process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "\n",
    "# process_names without OP07\n",
    "process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "\n",
    "labels = [\"good\",\"bad\"]\n",
    "\n",
    "path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "    data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "    data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "    X_data.extend(data_list)\n",
    "    y_data.extend(data_label)\n",
    "\n",
    "y_data_label = [item.split('_')[-1] for item in y_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(signal, wavelet='coif8', max_level=3):\n",
    "    \"\"\"\n",
    "    Perform wavelet packet decomposition and extract statistical features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : ndarray\n",
    "        Input signal (1D array)\n",
    "    wavelet : str\n",
    "        Wavelet to use (default: 'coif8')\n",
    "    max_level : int\n",
    "        Maximum decomposition level\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary of statistical features\n",
    "    \"\"\"\n",
    "    # Create wavelet packet\n",
    "    wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, mode='symmetric', maxlevel=max_level)\n",
    "    \n",
    "    # Extract nodes at the maximum level\n",
    "    level_nodes = [node.path for node in wp.get_level(max_level, 'natural')]\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for node in level_nodes:\n",
    "        # Get coefficients for this node\n",
    "        coeffs = wp[node].data\n",
    "        \n",
    "        # Extract statistical features\n",
    "        features[f\"mean_{node}\"] = np.mean(coeffs)\n",
    "        features[f\"max_{node}\"] = np.max(coeffs)\n",
    "        features[f\"min_{node}\"] = np.min(coeffs)\n",
    "        features[f\"std_{node}\"] = np.std(coeffs)\n",
    "        features[f\"kurtosis_{node}\"] = stats.kurtosis(coeffs)\n",
    "        features[f\"skewness_{node}\"] = stats.skew(coeffs)\n",
    "        \n",
    "        # Shannon entropy\n",
    "        # Normalize the coefficients\n",
    "        coeffs_norm = np.abs(coeffs) / np.sum(np.abs(coeffs) + 1e-10)\n",
    "        entropy = -np.sum(coeffs_norm * np.log2(coeffs_norm + 1e-10))\n",
    "        features[f\"entropy_{node}\"] = entropy\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "# List to store all features\n",
    "all_features = []\n",
    "\n",
    "# Process each sample in X_data\n",
    "for i, sample in enumerate(tqdm(X_data, desc=\"Extracting features\")):\n",
    "    sample_features = {}\n",
    "    \n",
    "    # Process each axis (channel)\n",
    "    for axis in range(sample.shape[1]):\n",
    "        # Get signal for this axis\n",
    "        signal = sample[:, axis]\n",
    "        \n",
    "        # Apply wavelet packet transform and extract features\n",
    "        wp_features = extract_wavelet_features(signal, wavelet='coif8', max_level=3)\n",
    "        \n",
    "        # Add axis identifier to feature names\n",
    "        for key, value in wp_features.items():\n",
    "            sample_features[f\"axis{axis}_{key}\"] = value\n",
    "    \n",
    "    # Add label\n",
    "    split_label = y_data[i].split(\"_\")\n",
    "    sample_features['label'] = split_label[-1]\n",
    "    sample_features['machine'] = split_label[0]\n",
    "    sample_features['process'] = split_label[1]\n",
    "    \n",
    "    # Add to collection\n",
    "    all_features.append(sample_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# display(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features, y_labels = transform_data(X_train, y_train, include_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Separate features and target\n",
    "X = features_df.drop(['label','machine'], axis=1)\n",
    "y = features_df['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(\n",
    "    model.feature_importances_,\n",
    "    index=X.columns,\n",
    "    columns=['importance']\n",
    ").sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importances.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, class_names=['good', 'bad']):\n",
    "    \"\"\"\n",
    "    Maakt en toont een confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: array-like, werkelijke labels.\n",
    "    - y_pred: array-like, voorspelde labels.\n",
    "    - class_names: lijst met class namen (optioneel). Als None, worden standaard numerieke labels gebruikt.\n",
    "    \"\"\"\n",
    "    # Bereken de confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Als er geen class_names zijn meegegeven, gebruik dan standaard numerieke labels.\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(cm.shape[0])]\n",
    "    \n",
    "    # Maak de plot aan\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Stel de ticks en labels in\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='Werkelijke label',\n",
    "           xlabel='Voorspelde label')\n",
    "    \n",
    "    # Draai de x-labels voor een betere leesbaarheid\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Voeg waarden toe aan de cellen van de matrix\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
