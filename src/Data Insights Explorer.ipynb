{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS Thesis - Koen de Bonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:57:31.567105Z",
     "start_time": "2025-03-25T13:57:31.562498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added c:\\Thesis\\DSS_Thesis_CNC\\DSS_Thesis_CNC to Python path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set the root directory to the parent of the current directory\n",
    "root_dir = Path(current_dir).parent\n",
    "\n",
    "# Add the root directory to sys.path so Python can find the utils module\n",
    "sys.path.append(str(root_dir))\n",
    "print(f\"Added {root_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:57:31.533907Z",
     "start_time": "2025-03-25T13:57:31.443496Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import data_loader_utils\n",
    "from utils.feature_extraction import transform_data, prepare_train_test_data\n",
    "import itertools \n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pywt\n",
    "import numpy as np\n",
    "from scipy import signal,stats\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     14\u001b[39m     y_data.extend(data_label)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# y_data_label = [item.split('_')[-1] for item in y_data]\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2872\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2868\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2870\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2872\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(cv.split(X=arrays[\u001b[32m0\u001b[39m], y=stratify))\n\u001b[32m   2874\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2877\u001b[39m     chain.from_iterable(\n\u001b[32m   2878\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2879\u001b[39m     )\n\u001b[32m   2880\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1909\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1879\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1880\u001b[39m \n\u001b[32m   1881\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1906\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1907\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1908\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1909\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2318\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2316\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2318\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2323\u001b[39m     )\n\u001b[32m   2325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2329\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "machines = [\"M01\",\"M02\",\"M03\"]\n",
    "process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "labels = [\"good\",\"bad\"]\n",
    "\n",
    "path_to_dataset = os.path.join(root_dir, \"data\")\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for process_name, machine, label in itertools.product(process_names, machines, labels):\n",
    "    data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "    data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "    X_data.extend(data_list)\n",
    "    y_data.extend(data_label)\n",
    "\n",
    "# y_data_label = [item.split('_')[-1] for item in y_data]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size=0.3,stratify=y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(signal, wavelet='coif8', max_level=3):\n",
    "    \"\"\"\n",
    "    Perform wavelet packet decomposition and extract statistical features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : ndarray\n",
    "        Input signal (1D array)\n",
    "    wavelet : str\n",
    "        Wavelet to use (default: 'coif8')\n",
    "    max_level : int\n",
    "        Maximum decomposition level\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary of statistical features\n",
    "    \"\"\"\n",
    "    # Create wavelet packet\n",
    "    wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, mode='symmetric', maxlevel=max_level)\n",
    "    \n",
    "    # Extract nodes at the maximum level\n",
    "    level_nodes = [node.path for node in wp.get_level(max_level, 'natural')]\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for node in level_nodes:\n",
    "        # Get coefficients for this node\n",
    "        coeffs = wp[node].data\n",
    "        \n",
    "        # Extract statistical features\n",
    "        features[f\"mean_{node}\"] = np.mean(coeffs)\n",
    "        features[f\"max_{node}\"] = np.max(coeffs)\n",
    "        features[f\"min_{node}\"] = np.min(coeffs)\n",
    "        features[f\"std_{node}\"] = np.std(coeffs)\n",
    "        features[f\"kurtosis_{node}\"] = stats.kurtosis(coeffs)\n",
    "        features[f\"skewness_{node}\"] = stats.skew(coeffs)\n",
    "        \n",
    "        # Shannon entropy\n",
    "        # Normalize the coefficients\n",
    "        coeffs_norm = np.abs(coeffs) / np.sum(np.abs(coeffs) + 1e-10)\n",
    "        entropy = -np.sum(coeffs_norm * np.log2(coeffs_norm + 1e-10))\n",
    "        features[f\"entropy_{node}\"] = entropy\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "# List to store all features\n",
    "all_features = []\n",
    "\n",
    "# Process each sample in X_data\n",
    "for i, sample in enumerate(tqdm(X_data, desc=\"Extracting features\")):\n",
    "    sample_features = {}\n",
    "    \n",
    "    # Process each axis (channel)\n",
    "    for axis in range(sample.shape[1]):\n",
    "        # Get signal for this axis\n",
    "        signal = sample[:, axis]\n",
    "        \n",
    "        # Apply wavelet packet transform and extract features\n",
    "        wp_features = extract_wavelet_features(signal, wavelet='coif8', max_level=3)\n",
    "        \n",
    "        # Add axis identifier to feature names\n",
    "        for key, value in wp_features.items():\n",
    "            sample_features[f\"axis{axis}_{key}\"] = value\n",
    "    \n",
    "    # Add label\n",
    "    split_label = y_data[i].split(\"_\")\n",
    "    sample_features['label'] = split_label[-1]\n",
    "    sample_features['machine'] = split_label[0]\n",
    "    sample_features['process'] = split_label[1]\n",
    "    \n",
    "    # Add to collection\n",
    "    all_features.append(sample_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# display(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  39%|███▉      | 470/1191 [00:15<00:23, 31.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_features, y_labels = \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Thesis\\DSS_Thesis_CNC\\DSS_Thesis_CNC\\utils\\feature_extraction.py:89\u001b[39m, in \u001b[36mtransform_data\u001b[39m\u001b[34m(X_data, y_data, include_metadata, wavelet, max_level)\u001b[39m\n\u001b[32m     86\u001b[39m signal = sample[:, axis]\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Apply wavelet packet transform and extract features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m wp_features = \u001b[43mextract_wavelet_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Add axis identifier to feature names\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m wp_features.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Thesis\\DSS_Thesis_CNC\\DSS_Thesis_CNC\\utils\\feature_extraction.py:28\u001b[39m, in \u001b[36mextract_wavelet_features\u001b[39m\u001b[34m(signal, wavelet, max_level)\u001b[39m\n\u001b[32m     25\u001b[39m wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, mode=\u001b[33m'\u001b[39m\u001b[33msymmetric\u001b[39m\u001b[33m'\u001b[39m, maxlevel=max_level)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Extract nodes at the maximum level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m level_nodes = [node.path \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnatural\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m     30\u001b[39m features = {}\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m level_nodes:\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Get coefficients for this node\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:798\u001b[39m, in \u001b[36mWaveletPacket.get_level\u001b[39m\u001b[34m(self, level, order, decompose)\u001b[39m\n\u001b[32m    795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecompose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m order == \u001b[33m\"\u001b[39m\u001b[33mnatural\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:383\u001b[39m, in \u001b[36mBaseNode.walk\u001b[39m\u001b[34m(self, func, args, kwargs, decompose)\u001b[39m\n\u001b[32m    381\u001b[39m subnode = \u001b[38;5;28mself\u001b[39m.get_subnode(part, decompose)\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subnode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[43msubnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecompose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:381\u001b[39m, in \u001b[36mBaseNode.walk\u001b[39m\u001b[34m(self, func, args, kwargs, decompose)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.level < \u001b[38;5;28mself\u001b[39m.maxlevel:\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.PARTS:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m         subnode = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_subnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecompose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m subnode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    383\u001b[39m             subnode.walk(func, args, kwargs, decompose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:230\u001b[39m, in \u001b[36mBaseNode.get_subnode\u001b[39m\u001b[34m(self, part, decompose)\u001b[39m\n\u001b[32m    228\u001b[39m subnode = \u001b[38;5;28mself\u001b[39m._get_node(part)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subnode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decompose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_empty:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     subnode = \u001b[38;5;28mself\u001b[39m._get_node(part)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m subnode\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:185\u001b[39m, in \u001b[36mBaseNode.decompose\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03mDecompose node data creating DWT coefficients subnodes.\u001b[39;00m\n\u001b[32m    174\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.level < \u001b[38;5;28mself\u001b[39m.maxlevel:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMaximum decomposition level reached.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_wavelet_packets.py:446\u001b[39m, in \u001b[36mNode._decompose\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[38;5;28mself\u001b[39m._create_subnode(\u001b[38;5;28mself\u001b[39m.D, data_d)\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     data_a, data_d = \u001b[43mdwt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                         \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28mself\u001b[39m._create_subnode(\u001b[38;5;28mself\u001b[39m.A, data_a)\n\u001b[32m    449\u001b[39m     \u001b[38;5;28mself\u001b[39m._create_subnode(\u001b[38;5;28mself\u001b[39m.D, data_d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koenb\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pywt\\_dwt.py:182\u001b[39m, in \u001b[36mdwt\u001b[39m\u001b[34m(data, wavelet, mode, axis)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AxisError(\u001b[33m\"\u001b[39m\u001b[33mAxis greater than data dimensions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     cA, cD = \u001b[43mdwt_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# TODO: Check whether this makes a copy\u001b[39;00m\n\u001b[32m    184\u001b[39m     cA, cD = np.asarray(cA, dt), np.asarray(cD, dt)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X_features, y_labels = transform_data(X_train, y_train, include_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Separate features and target\n",
    "X = features_df.drop(['label','machine'], axis=1)\n",
    "y = features_df['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(\n",
    "    model.feature_importances_,\n",
    "    index=X.columns,\n",
    "    columns=['importance']\n",
    ").sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importances.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, class_names=['good', 'bad']):\n",
    "    \"\"\"\n",
    "    Maakt en toont een confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: array-like, werkelijke labels.\n",
    "    - y_pred: array-like, voorspelde labels.\n",
    "    - class_names: lijst met class namen (optioneel). Als None, worden standaard numerieke labels gebruikt.\n",
    "    \"\"\"\n",
    "    # Bereken de confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Als er geen class_names zijn meegegeven, gebruik dan standaard numerieke labels.\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(cm.shape[0])]\n",
    "    \n",
    "    # Maak de plot aan\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Stel de ticks en labels in\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='Werkelijke label',\n",
    "           xlabel='Voorspelde label')\n",
    "    \n",
    "    # Draai de x-labels voor een betere leesbaarheid\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Voeg waarden toe aan de cellen van de matrix\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
